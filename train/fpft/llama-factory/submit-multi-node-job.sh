#!/bin/bash
TS=$(date -u '+%Y%m%d%H%M%S')

export JOB_NAME=${JOB_NAME:-sample-torch-job-$TS}
export NNODES=${NNODES:-2}
export WORKER_NODES=$((NNODES - 1))
export MODEL_NAME=${MODEL_NAME:-Meta-Llama-3.1-8B-Instruct}
export GPUS_PER_NODE=${GPUS_PER_NODE:-2}
let DEFAULT_CPU_RESOURCES_REQS=$((8 * ${GPUS_PER_NODE}))
let DEFAULT_CPU_RESOURCES_LIMITS=$((10 * ${GPUS_PER_NODE}))
let DEFAULT_MEM_RESOURCES_REQS=$((16 * ${GPUS_PER_NODE}))
let DEFAULT_MEM_RESOURCES_LIMITS=$((32 * ${GPUS_PER_NODE}))
export EPOCHS=${EPOCHS:-2}
export TRAIN_BATCH_SIZE_PER_DEVICE=${TRAIN_BATCH_SIZE_PER_DEVICE:-2}
export DATASET_TEMPLATE=${DATASET_TEMPLATE:-llama3}
export REPORT_TO=${REPORT_TO:-tensorboard}
export LOGGING_DIR=${LOGGING_DIR:-/app/output/logs}
export HCA_SHARED_DEVICES=${GPUS_PER_NODE}
export CPU_RESOURCES_REQS=${CPU_RESOURCES_REQS:-${DEFAULT_CPU_RESOURCES_REQS}}
export CPU_RESOURCES_LIMITS=${CPU_RESOURCES_LIMITS:-${DEFAULT_CPU_RESOURCES_LIMITS}}
export MEM_RESOURCES_REQS=${MEM_RESOURCES_REQS_:-${DEFAULT_MEM_RESOURCES_REQS}Gi}
export MEM_RESOURCES_LIMITS=${MEM_RESOURCES_LIMITS:-${DEFAULT_MEM_RESOURCES_LIMITS}Gi}
export GPU_SPEC=${GPU_SPEC:-A800_NVLINK_80GB}

envsubst '$JOB_NAME $MODEL_NAME $NNODES $WORKER_NODES $GPUS_PER_NODE $EPOCHS $TRAIN_BATCH_SIZE_PER_DEVICE $DATASET_TEMPLATE $REPORT_TO $LOGGING_DIR $GPU_SPEC $HCA_SHARED_DEVICES $CPU_RESOURCES_REQS $MEM_RESOURCES_REQS $CPU_RESOURCES_LIMITS $MEM_RESOURCES_LIMITS' < lora-example-pytorchjob.tpl.yaml | kubectl replace --force -f -

